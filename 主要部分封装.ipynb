{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#先把csv中一个句子拿出来  然后把每个单词去词训练库里面找对应的向量规定对于每个句子矩阵都构建一个2000,300的矩阵。\n",
    "def hang_vectoes_label(index): # 要读取csv文本第几行的文章数据 会返回一个 列表里面是文章的词向量矩阵和标签值\n",
    "    row=df.values[index] #读取数据的第几行\n",
    "    row=row[0].split()   #这些数据变成一个列表\n",
    "    label=row[0]\n",
    "    hang_txt=row[1:]\n",
    "    hang_labeltxt_juzhen=[]\n",
    "    meihang_wenben_juzhen=np.zeros((2000,300))#，每一个文本定义一个空的全0的2000,300的矩阵 \n",
    "    t=0\n",
    "    for a in hang_txt:\n",
    "        try:\n",
    "            vec = new_model[a]  #从词库里面取出这些词对应的向量 \n",
    "        except KeyError:        #当在词向量模型中没有这个单词的向量的时候可以设置错误跳过 然后把这个单词的向量设置为全0\n",
    "            vec=np.zeros((1,300))\n",
    "        meihang_wenben_juzhen[int(t)]=vec\n",
    "        if t ==1999:\n",
    "            break\n",
    "        t=t+1\n",
    "    hang_labeltxt_juzhen.append(label)\n",
    "    hang_labeltxt_juzhen.append(meihang_wenben_juzhen)\n",
    "    return hang_labeltxt_juzhen\n",
    "# a=hang_vectoes_label(0)  #返回一个列表 第一个是行文本的分类的label 第二个是行文本 2000*300 的词向量矩阵 \n",
    "# # 这个原始数据有1730条文本 数据 \n",
    "\n",
    "\n",
    "\n",
    "#手动对数据分块 并且把数据加载到dataloader 让dataloader分batch\n",
    "def dataloader_batch(start,end,batch,shuffle):# 把多少行的数据加入到这个data.TensorDataset中 然后在对这个中的这些行数据分batch训练\n",
    "            #     开始     终止 batch大小  是否打乱 True False \n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(int(start),int(end),1):\n",
    "        a=hang_vectoes_label(i)\n",
    "        x_1=a[1]\n",
    "        y_1=int(a[0])\n",
    "        x.append(x_1)\n",
    "        y.append(y_1)\n",
    "    x=torch.Tensor(x)\n",
    "    y=torch.Tensor(y)\n",
    "    torch_dataset = data.TensorDataset(x, y) #要求这两部分 第一个维度是相同的 \n",
    "    loader = data.DataLoader(\n",
    "        dataset=torch_dataset,\n",
    "        batch_size=batch,             # 每批提取的数量\n",
    "        shuffle=shuffle,             # 要不要打乱数据（打乱比较好）\n",
    "        num_workers=0)            # 多少线程来读取数据\n",
    "    return loader \n",
    "\n",
    "\n",
    "\n",
    "#定义模型优化器\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiRNN, self).__init__()\n",
    "        \n",
    "        self.embed_size=300  #样本得输入维度 \n",
    "        self.num_hiddens=100 # 隐藏层神经元的个数\n",
    "        self.num_layers=2\n",
    "        self.encoder = nn.LSTM(input_size=self.embed_size, \n",
    "                                hidden_size=self.num_hiddens, \n",
    "                                num_layers=self.num_layers,\n",
    "                                bidirectional=True)\n",
    "        \n",
    "        self.decoder = nn.Linear(4*self.num_hiddens, 14)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.reshape(2000,16,300)\n",
    "        #                batch大小\n",
    "#         print(x.shape)\n",
    "        x=torch.Tensor(x)\n",
    "        outputs, _ = self.encoder(x) \n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        outs = F.sigmoid(outs)\n",
    "        return outs\n",
    "\n",
    "model = BiRNN()\n",
    "# model.aux_logits=False\n",
    "criterion = torch.nn.CrossEntropyLoss()  # 损失函数的计算 交叉熵损失函数计算\n",
    "# optimizer=optim.SGD(model.parameters(),lr=0.01,momentum=0.5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 优化器\n",
    "# momentum是优化梯度的方向自动适应学习率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove2word2vec(\"text_train_glove_vectors.txt\", \"text_train_vectors_wd2.txt\") #\n",
    "#把glove训练好的文件转成Wordvec可以读的 如果转过了这个就注释掉 \n",
    "new_model = KeyedVectors.load_word2vec_format('text_train_vectors_wd2.txt', binary=False)\n",
    "\n",
    "df=pd.read_csv(\"train_set.csv\")\n",
    "# with open(\"train_set.csv\", 'r') as f: #计算长度\n",
    "#     hang_count=len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个16条数据内\n",
      "第0个320条数据内 epoch世代: 0 第0个batch: 这个batch的损失值 2.638547658920288\n",
      "第0个320条数据内 epoch世代: 1 第0个batch: 这个batch的损失值 2.6371188163757324\n",
      "第0个320条数据内 epoch世代: 2 第0个batch: 这个batch的损失值 2.6349856853485107\n",
      "第0个320条数据内 epoch世代: 3 第0个batch: 这个batch的损失值 2.6338651180267334\n",
      "第0个320条数据内 epoch世代: 4 第0个batch: 这个batch的损失值 2.6308698654174805\n",
      "第0个320条数据内 epoch世代: 5 第0个batch: 这个batch的损失值 2.626453399658203\n",
      "第0个320条数据内 epoch世代: 6 第0个batch: 这个batch的损失值 2.6231188774108887\n",
      "第0个320条数据内 epoch世代: 7 第0个batch: 这个batch的损失值 2.6184072494506836\n",
      "第0个320条数据内 epoch世代: 8 第0个batch: 这个batch的损失值 2.6135809421539307\n",
      "第0个320条数据内 epoch世代: 9 第0个batch: 这个batch的损失值 2.6054399013519287\n",
      "第0个320条数据内 epoch世代: 10 第0个batch: 这个batch的损失值 2.5990264415740967\n",
      "第0个320条数据内 epoch世代: 11 第0个batch: 这个batch的损失值 2.590970754623413\n",
      "第0个320条数据内 epoch世代: 12 第0个batch: 这个batch的损失值 2.58365535736084\n",
      "第0个320条数据内 epoch世代: 13 第0个batch: 这个batch的损失值 2.576279878616333\n",
      "第0个320条数据内 epoch世代: 14 第0个batch: 这个batch的损失值 2.5667366981506348\n",
      "第0个320条数据内 epoch世代: 15 第0个batch: 这个batch的损失值 2.558185338973999\n",
      "第0个320条数据内 epoch世代: 16 第0个batch: 这个batch的损失值 2.5476937294006348\n",
      "第0个320条数据内 epoch世代: 17 第0个batch: 这个batch的损失值 2.5350587368011475\n",
      "第0个320条数据内 epoch世代: 18 第0个batch: 这个batch的损失值 2.524035930633545\n",
      "第0个320条数据内 epoch世代: 19 第0个batch: 这个batch的损失值 2.517559766769409\n",
      "第0个320条数据内 epoch世代: 20 第0个batch: 这个batch的损失值 2.5045437812805176\n",
      "第0个320条数据内 epoch世代: 21 第0个batch: 这个batch的损失值 2.4944491386413574\n",
      "第0个320条数据内 epoch世代: 22 第0个batch: 这个batch的损失值 2.48315691947937\n",
      "第0个320条数据内 epoch世代: 23 第0个batch: 这个batch的损失值 2.4729979038238525\n",
      "第0个320条数据内 epoch世代: 24 第0个batch: 这个batch的损失值 2.463487386703491\n",
      "第0个320条数据内 epoch世代: 25 第0个batch: 这个batch的损失值 2.451368808746338\n",
      "第0个320条数据内 epoch世代: 26 第0个batch: 这个batch的损失值 2.4460697174072266\n",
      "第0个320条数据内 epoch世代: 27 第0个batch: 这个batch的损失值 2.435849666595459\n",
      "第0个320条数据内 epoch世代: 28 第0个batch: 这个batch的损失值 2.428556442260742\n",
      "第0个320条数据内 epoch世代: 29 第0个batch: 这个batch的损失值 2.4227051734924316\n",
      "第0个320条数据内 epoch世代: 30 第0个batch: 这个batch的损失值 2.4144983291625977\n",
      "第0个320条数据内 epoch世代: 31 第0个batch: 这个batch的损失值 2.407684326171875\n",
      "第0个320条数据内 epoch世代: 32 第0个batch: 这个batch的损失值 2.40315580368042\n",
      "第0个320条数据内 epoch世代: 33 第0个batch: 这个batch的损失值 2.398272752761841\n",
      "第0个320条数据内 epoch世代: 34 第0个batch: 这个batch的损失值 2.394757032394409\n",
      "第0个320条数据内 epoch世代: 35 第0个batch: 这个batch的损失值 2.3908801078796387\n",
      "第0个320条数据内 epoch世代: 36 第0个batch: 这个batch的损失值 2.3880887031555176\n",
      "第0个320条数据内 epoch世代: 37 第0个batch: 这个batch的损失值 2.3867852687835693\n",
      "第0个320条数据内 epoch世代: 38 第0个batch: 这个batch的损失值 2.385093927383423\n",
      "第0个320条数据内 epoch世代: 39 第0个batch: 这个batch的损失值 2.3830339908599854\n",
      "第0个320条数据内 epoch世代: 40 第0个batch: 这个batch的损失值 2.3827097415924072\n",
      "第0个320条数据内 epoch世代: 41 第0个batch: 这个batch的损失值 2.3817808628082275\n",
      "第0个320条数据内 epoch世代: 42 第0个batch: 这个batch的损失值 2.3813624382019043\n",
      "第0个320条数据内 epoch世代: 43 第0个batch: 这个batch的损失值 2.3810336589813232\n",
      "第0个320条数据内 epoch世代: 44 第0个batch: 这个batch的损失值 2.380376100540161\n",
      "第0个320条数据内 epoch世代: 45 第0个batch: 这个batch的损失值 2.3798885345458984\n",
      "第0个320条数据内 epoch世代: 46 第0个batch: 这个batch的损失值 2.379645586013794\n",
      "第0个320条数据内 epoch世代: 47 第0个batch: 这个batch的损失值 2.3796095848083496\n",
      "第0个320条数据内 epoch世代: 48 第0个batch: 这个batch的损失值 2.3791861534118652\n",
      "第0个320条数据内 epoch世代: 49 第0个batch: 这个batch的损失值 2.379406452178955\n",
      "第0个320条数据内 epoch世代: 50 第0个batch: 这个batch的损失值 2.379249095916748\n",
      "第0个320条数据内 epoch世代: 51 第0个batch: 这个batch的损失值 2.3793692588806152\n",
      "第0个320条数据内 epoch世代: 52 第0个batch: 这个batch的损失值 2.379006862640381\n",
      "第0个320条数据内 epoch世代: 53 第0个batch: 这个batch的损失值 2.3800759315490723\n",
      "第0个320条数据内 epoch世代: 54 第0个batch: 这个batch的损失值 2.380007266998291\n",
      "第0个320条数据内 epoch世代: 55 第0个batch: 这个batch的损失值 2.3795394897460938\n",
      "第0个320条数据内 epoch世代: 82 第0个batch: 这个batch的损失值 2.3833844661712646\n",
      "第0个320条数据内 epoch世代: 83 第0个batch: 这个batch的损失值 2.3837509155273438\n",
      "第0个320条数据内 epoch世代: 84 第0个batch: 这个batch的损失值 2.3835630416870117\n",
      "第0个320条数据内 epoch世代: 85 第0个batch: 这个batch的损失值 2.3835620880126953\n",
      "第0个320条数据内 epoch世代: 86 第0个batch: 这个batch的损失值 2.3834173679351807\n",
      "第0个320条数据内 epoch世代: 87 第0个batch: 这个batch的损失值 2.383424997329712\n",
      "第0个320条数据内 epoch世代: 89 第0个batch: 这个batch的损失值 2.383542537689209\n",
      "第0个320条数据内 epoch世代: 90 第0个batch: 这个batch的损失值 2.3833212852478027\n",
      "第0个320条数据内 epoch世代: 91 第0个batch: 这个batch的损失值 2.383453369140625\n",
      "第0个320条数据内 epoch世代: 92 第0个batch: 这个batch的损失值 2.3831253051757812\n",
      "第0个320条数据内 epoch世代: 93 第0个batch: 这个batch的损失值 2.3831236362457275\n",
      "第0个320条数据内 epoch世代: 94 第0个batch: 这个batch的损失值 2.3827404975891113\n",
      "第0个320条数据内 epoch世代: 95 第0个batch: 这个batch的损失值 2.3829736709594727\n",
      "第0个320条数据内 epoch世代: 96 第0个batch: 这个batch的损失值 2.3830177783966064\n",
      "第0个320条数据内 epoch世代: 97 第0个batch: 这个batch的损失值 2.382502317428589\n",
      "第0个320条数据内 epoch世代: 98 第0个batch: 这个batch的损失值 2.3823792934417725\n",
      "第0个320条数据内 epoch世代: 99 第0个batch: 这个batch的损失值 2.3820717334747314\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1): #全部是6\n",
    "    print(\"第\"+str(i)+\"个16条数据内\")\n",
    "    loader=dataloader_batch(i*16,i*16+16,16,True) \n",
    "    if __name__ == '__main__':\n",
    "        for epoch in range(100):    # 对整套数据训练3次\n",
    "             for step, (batch_x, batch_y) in enumerate(loader):  # 每一步loader释放一小批数据用来学习\n",
    "                    input=batch_x\n",
    "                    target=batch_y\n",
    "                    outputs=model(input)#经过这个模型输出的是 如果批大小是4 就输出四行 每行是14个分类的 二维tensor ,\n",
    "                    target =target.long() #转化成长整形张量进行计算 loss\n",
    "                    loss = criterion(outputs, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    print(\"第\"+str(i)+\"个320条数据内\",\"epoch世代:\", epoch, \"第\"+str(step)+\"个batch:\",\"这个batch的损失值\" ,loss.item())                    \n",
    "#保存训练模型\n",
    "torch.save(model.state_dict(), 'trainModel.pth')\n",
    "#模型加载：\n",
    "# model = BiRNN()\n",
    "# model.load_state_dict(torch.load('trainModel.pth'))\n",
    "# model.eval()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
